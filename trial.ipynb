{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Hold the background frame for background subtraction.\n",
    "background = None\n",
    "# Hold the hand's data so all its details are in one place.\n",
    "hand = None\n",
    "# Variables to count how many frames have passed and to set the size of the window.\n",
    "frames_elapsed = 0\n",
    "FRAME_HEIGHT = 200\n",
    "FRAME_WIDTH = 300\n",
    "# Humans come in a ton of beautiful shades and colors.\n",
    "# Try editing these if your program has trouble recognizing your skin tone.\n",
    "CALIBRATION_TIME = 30\n",
    "BG_WEIGHT = 0.5\n",
    "OBJ_THRESHOLD = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandData:\n",
    "    top = (0,0)\n",
    "    bottom = (0,0)\n",
    "    left = (0,0)\n",
    "    right = (0,0)\n",
    "    centerX = 0\n",
    "    prevCenterX = 0\n",
    "    isInFrame =False\n",
    "    isWaving = False\n",
    "    fingers = 0\n",
    "\n",
    "    def __init__(self, top, bottom, left, right, centerX):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.centerX = centerX\n",
    "        self.prevCenterX = 0\n",
    "        isWaving = False\n",
    "        self.isInFrame = False\n",
    "    \n",
    "    def update(self, top, bottom, left, right):\n",
    "        self.top = top\n",
    "        self.bottom = bottom\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take the current frame, the number of frames elapsed, and how many fingers we've detected\n",
    "# so we can print on the screen which gesture is happening (or if the camera is calibrating).\n",
    "\n",
    "def write_on_image(frame, hand):\n",
    "    text = \"Searching...\"\n",
    "    \n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        text = \"Calibrating...\"\n",
    "    elif hand == None or hand.isInFrame == False:\n",
    "        text = \"No hand detected.\"\n",
    "    else:\n",
    "        if hand.isWaving:\n",
    "            text = \"Waving\"\n",
    "        elif hand.fingers == 0:\n",
    "            text = \"Rock\"\n",
    "        elif hand.fingers == 1:\n",
    "            text = \"Pointing\"\n",
    "        elif hand.fingers == 2:\n",
    "            text = \"Scissors\"\n",
    "        elif hand.fingers == 3:\n",
    "            text = \"Three\"\n",
    "        elif hand.fingers == 4:\n",
    "            text = \"Four\"\n",
    "        elif hand.fingers == 5:\n",
    "            text = \"Paper\"  \n",
    "    \n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "    cv2.putText(frame, text, (10, 20), cv2.FONT_HERSHEY_COMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.rectangle(frame, (region_left, region_top), (region_right, region_bottom), (255, 255, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(frame):\n",
    "    #separating region of interest from the rest of the frame\n",
    "    region = frame[region_top:region_bottom, region_left:region_right]\n",
    "    #make it grayscale so we can detect the edges moore easily\n",
    "    region = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)\n",
    "    # Use a Gaussian blur to prevent frame noise from being labeled as an edge.\n",
    "    region = cv2.GaussianBlur(region, (5,5), 0)\n",
    "\n",
    "    return region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(region):\n",
    "    # We have to use the global keyword because we want to edit the global variable.\n",
    "    global background\n",
    "    # If we haven't captured the background yet, make the current region the background.\n",
    "    if background is None:\n",
    "        background = region.copy().astype(\"float\")\n",
    "        return\n",
    "    # Otherwise, add this captured frame to the average of the backgrounds.\n",
    "    cv2.accumulateWeighted(region, background, BG_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use differencing to separate the background from the object of interest.\n",
    "def segment(region):\n",
    "    global hand\n",
    "    # Find the absolute difference between the background and the current frame.\n",
    "    diff = cv2.absdiff(background.astype(np.uint8), region)\n",
    "\n",
    "    # Threshold that region with a strict 0 or 1 ruling so only the foreground remains.\n",
    "    thresholded_region = cv2.threshold(diff, OBJ_THRESHOLD, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Get the contours of the region, which will return an outline of the hand.\n",
    "    (contours, _) = cv2.findContours(thresholded_region.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # If we didn't get anything, there's no hand.\n",
    "    if len(contours) == 0:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = False\n",
    "        return\n",
    "    # Otherwise return a tuple of the filled hand (thresholded_region), along with the outline (segmented_region).\n",
    "    else:\n",
    "        if hand is not None:\n",
    "            hand.isInFrame = True\n",
    "        segmented_region = max(contours, key = cv2.contourArea)\n",
    "        return (thresholded_region, segmented_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "region_top = 0\n",
    "region_bottom = int(2 * FRAME_HEIGHT / 3)\n",
    "region_left = int(FRAME_WIDTH / 2)\n",
    "region_right = FRAME_WIDTH\n",
    "\n",
    "frames_elapsed = 0\n",
    "\n",
    "\n",
    "while (True):\n",
    "    # Store the frame from the video capture and resize it to the desired window size.\n",
    "    ret, frame = capture.read()\n",
    "    frame = cv2.resize(frame, (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    # Convert the frame to grayscale and blur it for easier detection.\n",
    "    region = get_region(frame)\n",
    "\n",
    "    if frames_elapsed < CALIBRATION_TIME:\n",
    "        get_average(region)\n",
    "    else:\n",
    "        region_pair = segment(region)\n",
    "        if region_pair is not None:\n",
    "            # If we have the regions segmented successfully, show them in another window for the user.\n",
    "            (thresholded_region, segmented_region) = region_pair\n",
    "            cv2.drawContours(region, [segmented_region], -1, (255, 255, 255))\n",
    "            cv2.imshow(\"Segmented Image\", region)\n",
    "            \n",
    "    write_on_image(frame, hand)\n",
    "\n",
    "    cv2.imshow(\"Camera Input\", frame)\n",
    "    frames_elapsed += 1\n",
    "    # Check if user wants to exit.\n",
    "    if (cv2.waitKey(1) & 0xFF == ord('x')):\n",
    "        break\n",
    "\n",
    "# When we exit the loop, we have to stop the capture too.\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
